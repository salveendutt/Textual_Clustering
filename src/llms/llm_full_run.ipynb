{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from langchain_ollama import OllamaLLM\n",
    "random.seed(42)\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_per_class = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_train = pd.read_csv('../../data/AG News/train.csv')\n",
    "\n",
    "class_to_text_mapping = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Science\"}\n",
    "text_to_class_mapping = {'World': 1, 'Sports': 2, 'Business': 3, 'Science': 4}\n",
    "ag_train['Class'] = ag_train['Class Index'].map(class_to_text_mapping)\n",
    "\n",
    "# CREATING A SAMPLE TEST SET\n",
    "ag_train_world_sample = ag_train[ag_train['Class Index'] == 1].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_sports_sample = ag_train[ag_train['Class Index'] == 2].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_business_sample = ag_train[ag_train['Class Index'] == 3].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_science_sample = ag_train[ag_train['Class Index'] == 4].sample(sample_size_per_class, random_state=42)\n",
    "\n",
    "# Combine the four dataframes of different categories\n",
    "ag_train_sample = pd.concat([ag_train_world_sample, \n",
    "                            ag_train_sports_sample, \n",
    "                            ag_train_business_sample, \n",
    "                            ag_train_science_sample], \n",
    "                           ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "ag_train_sample = ag_train_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Reset the index\n",
    "ag_train_sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull gemma3\n",
    "# !ollama pull gemma3:12b-it-qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OllamaLLM(model=\"gemma3:12b-it-qat\")\n",
    "llm = OllamaLLM(model=\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ag_train_sample['Class'].unique()\n",
    "\n",
    "def assign_topics(input_df, topics, llm=None, checkpoint_interval=300, noise_strategies=None):\n",
    "\n",
    "    output_filename = '../../data/AG News/train_from_llm.csv'\n",
    "\n",
    "    if llm is None:\n",
    "        raise ValueError(\"LLM instance must be provided\")\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # Initialize Topic column with 'Unknown'\n",
    "    df['Predicted Topic'] = 'Unknown'\n",
    "    \n",
    "    # Process each news item individually with progress bar\n",
    "    for idx in tqdm(range(len(df)), desc=\"Assigning topics\"):\n",
    "        news_item = df.iloc[idx]['Description']\n",
    "        \n",
    "        # Generate prompt for current item\n",
    "        prompt_assigning_prompt = f'''You are provided with news and helping to classify them based on the topics.\n",
    "Please assign the news to the topics provided. Return only the name of the topic for the respective news.\n",
    "News can be found in tripletick block: ```{news_item}```\n",
    "Topics to choose from: {topics}\n",
    "Please return ONLY the topic name. DO NOT OUTPUT any additional text, quotes, or formatting.'''\n",
    "        \n",
    "        try:\n",
    "            result = llm.invoke(prompt_assigning_prompt)\n",
    "            # Clean the result\n",
    "            result = result.strip()\n",
    "            if result.startswith('```') and result.endswith('```'):\n",
    "                result = result[3:-3].strip()\n",
    "            \n",
    "            # Update the DataFrame with the assigned topic\n",
    "            df.iloc[idx, df.columns.get_loc('Predicted Topic')] = result\n",
    "            \n",
    "            # Save checkpoint every checkpoint_interval rows\n",
    "            # if (idx + 1) % checkpoint_interval == 0:\n",
    "            #     checkpoint_filename = f'{output_filename}_{idx + 1}.csv'\n",
    "            #     df.to_csv(checkpoint_filename, index=False)\n",
    "            #     print(f\"\\nCheckpoint saved: {checkpoint_filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {idx}: {str(e)}\")\n",
    "            print(f\"Result received: {result}\")\n",
    "            continue\n",
    "    df['Predicted Topic Index'] = df['Predicted Topic'].map(text_to_class_mapping)\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nFinal results saved: {output_filename}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d960195c5460fb5708e2817017ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning topics:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results saved: ../../data/AG News/train_from_llm.csv\n"
     ]
    }
   ],
   "source": [
    "result = assign_topics(\n",
    "    ag_train_sample,\n",
    "    topics=str(topics),\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7550\n",
      "F1 Score: 0.7344\n",
      "Precision: 0.8082\n",
      "Recall: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Convert predicted topics to numeric labels using the mapping\n",
    "predicted_labels = result['Predicted Topic']\n",
    "true_labels = result['Class']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7750\n",
      "F1 Score: 0.7509\n",
      "Precision: 0.8212\n",
      "Recall: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Convert predicted topics to numeric labels using the mapping\n",
    "predicted_labels = result['Predicted Topic']\n",
    "true_labels = result['Class']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
