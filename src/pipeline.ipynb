{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Assumptions:\n",
    "* Business wants to classify incoming textual data\n",
    "* Minimal labeling needed. Zero to 20 labels per classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from topic_modeling_pipeline import *\n",
    "from classification_pipeline import *\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taking_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_test = pd.read_csv('../data/AG News/test.csv')\n",
    "ag_train = pd.read_csv('../data/AG News/train.csv')\n",
    "\n",
    "# CREATING A SAMPLE TEST SET\n",
    "if taking_sample == True:\n",
    "    ag_test_world_sample = ag_test[ag_test['Class Index'] == 1].sample(100, random_state=42)\n",
    "    ag_test_sports_sample = ag_test[ag_test['Class Index'] == 2].sample(100, random_state=42)\n",
    "    ag_test_business_sample = ag_test[ag_test['Class Index'] == 3].sample(100, random_state=42)\n",
    "    ag_test_science_sample = ag_test[ag_test['Class Index'] == 4].sample(100, random_state=42)\n",
    "\n",
    "    # Combine the four dataframes of different categories\n",
    "    ag_test_sample = pd.concat([ag_test_world_sample, ag_test_sports_sample, ag_test_business_sample, ag_test_science_sample])\n",
    "\n",
    "    # Shuffle the combined dataframe\n",
    "    ag_test_sample = ag_test_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Reset the index\n",
    "    ag_test_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ag_test_sample_desc = ag_test_sample['Description']\n",
    "    ag_test_sample_labels = ag_test_sample['Class Index']\n",
    "\n",
    "else:\n",
    "    ag_test_sample_desc = ag_test['Description']\n",
    "    ag_test_sample_labels = ag_test['Class Index']\n",
    "\n",
    "ag_news_sample = {\n",
    "    \"AG News\": (ag_test_sample_desc, ag_test_sample_labels),\n",
    "}\n",
    "\n",
    "\n",
    "# CREATING A BALANCED TRAINING SET\n",
    "ag_train_world_10 = ag_train[ag_train['Class Index'] == 1].sample(10, random_state=42)\n",
    "ag_train_sports_10 = ag_train[ag_train['Class Index'] == 2].sample(10, random_state=42)\n",
    "ag_train_business_10 = ag_train[ag_train['Class Index'] == 3].sample(10, random_state=42)\n",
    "ag_train_science_10 = ag_train[ag_train['Class Index'] == 4].sample(10, random_state=42)\n",
    "\n",
    "# Combine the four dataframes of different categories\n",
    "ag_news_baseline_combined = pd.concat([\n",
    "    ag_train_world_10, \n",
    "    ag_train_sports_10, \n",
    "    ag_train_business_10, \n",
    "    ag_train_science_10\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "ag_news_baseline_combined = ag_news_baseline_combined.sample(frac=1, random_state=42)\n",
    "\n",
    "# Reset the index\n",
    "ag_news_baseline_combined = ag_news_baseline_combined.reset_index(drop=True)\n",
    "\n",
    "ag_train_40_desc = ag_news_baseline_combined['Description']\n",
    "ag_train_40_labels = ag_news_baseline_combined['Class Index']\n",
    "ag_news_train = {\n",
    "    \"AG News\":(ag_train_40_desc, ag_train_40_labels),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_news_llm_train_df = pd.read_csv('../outputs/llm_to_label/news_assigned_final.csv')\n",
    "# ag_news_llm_train_df['predicted_label'] = ag_news_llm_train_df['predicted_label_text'].map(text_to_class_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# ag_news_llm_train_df = ag_news_llm_train_df[ag_news_llm_train_df['predicted_label'] != -1]  # Remove rows with -1 labels\n",
    "# ag_news_llm_train = {'AG News': (ag_news_llm_train_df['text'], ag_news_llm_train_df['predicted_label'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-24 18:02:41,063 TARS initialized without a task. You need to call .add_and_switch_to_new_task() before training this model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TARSFewShot']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_modelling_orchestrator = TopicModelingPipelineOrchestrator()\n",
    "topic_modelling_orchestrator.add_models_grid(\n",
    "    model_types=['LDA', 'LSI', 'NMF'],\n",
    "    param_grid={'n_topics': [4]}\n",
    ")\n",
    "\n",
    "class_orchestrator = ClassificationPipelineOrchestrator()\n",
    "class_orchestrator.add_models_grid(\n",
    "    model_types=[\n",
    "        # 'TARSZeroShot',\n",
    "        'TARSFewShot',\n",
    "        # 'SVM',\n",
    "        # 'XGBoost',\n",
    "        # 'RandomForest',\n",
    "        # 'LightGBM',\n",
    "        # 'SVMRoberta', \n",
    "        # 'XGBoostRoberta',\n",
    "        # 'LightGBMRoberta',\n",
    "    ]\n",
    "    # param_grid={'SVMRoberta': [{}], 'SVM': [{}]}  # Empty dictionary means default parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noise_strategy import *\n",
    "noise_strategies = [\n",
    "    NoNoise(),\n",
    "    # AddRandomCharsNoise(), \n",
    "    # AddRandomWordsNoise(), \n",
    "    # DeleteRandomWordsNoise(), \n",
    "    # ShuffleSentencesNoise(noise_level=0.7), \n",
    "    # ReplaceWithSynonymsNoise(), \n",
    "    # ReplaceWithAntonymsNoise()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_orchestrator.evaluate_with_training(ag_news_llm_train, ag_news_sample, noise_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593036906abd4a54980948b28d57f6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ba75b8dea646808b2f148ab6fe4fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e440b3ffde44c5f830c1e57b8422059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Noise Strategies:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ffb5e14cea4f3f92a95410ba9abe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Noise Strategies:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AG News</td>\n",
       "      <td>AddRandomCharsNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.548776</td>\n",
       "      <td>0.552725</td>\n",
       "      <td>0.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AG News</td>\n",
       "      <td>AddRandomCharsNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.695938</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.7350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AG News</td>\n",
       "      <td>AddRandomWordsNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.562756</td>\n",
       "      <td>0.568250</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AG News</td>\n",
       "      <td>AddRandomWordsNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>0.703401</td>\n",
       "      <td>0.701125</td>\n",
       "      <td>0.7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AG News</td>\n",
       "      <td>DeleteRandomWordsNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.542542</td>\n",
       "      <td>0.547450</td>\n",
       "      <td>0.5425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG News</td>\n",
       "      <td>DeleteRandomWordsNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.706675</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AG News</td>\n",
       "      <td>NoNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AG News</td>\n",
       "      <td>NoNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7525</td>\n",
       "      <td>0.722987</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.7525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ReplaceWithAntonymsNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.552548</td>\n",
       "      <td>0.557275</td>\n",
       "      <td>0.5525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ReplaceWithAntonymsNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7450</td>\n",
       "      <td>0.712959</td>\n",
       "      <td>0.708975</td>\n",
       "      <td>0.7450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ReplaceWithSynonymsNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.558023</td>\n",
       "      <td>0.562525</td>\n",
       "      <td>0.5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ReplaceWithSynonymsNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7425</td>\n",
       "      <td>0.713238</td>\n",
       "      <td>0.712600</td>\n",
       "      <td>0.7425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ShuffleSentencesNoise</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AG News</td>\n",
       "      <td>ShuffleSentencesNoise</td>\n",
       "      <td>TARSZeroShot</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.720750</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset                     Noise         Model  Accuracy  F1 Score  \\\n",
       "8   AG News       AddRandomCharsNoise           SVM    0.5500  0.548776   \n",
       "1   AG News       AddRandomCharsNoise  TARSZeroShot    0.7350  0.695938   \n",
       "9   AG News       AddRandomWordsNoise           SVM    0.5625  0.562756   \n",
       "2   AG News       AddRandomWordsNoise  TARSZeroShot    0.7325  0.703401   \n",
       "10  AG News    DeleteRandomWordsNoise           SVM    0.5425  0.542542   \n",
       "3   AG News    DeleteRandomWordsNoise  TARSZeroShot    0.7375  0.706675   \n",
       "7   AG News                   NoNoise           SVM    0.5575  0.556953   \n",
       "0   AG News                   NoNoise  TARSZeroShot    0.7525  0.722987   \n",
       "13  AG News  ReplaceWithAntonymsNoise           SVM    0.5525  0.552548   \n",
       "6   AG News  ReplaceWithAntonymsNoise  TARSZeroShot    0.7450  0.712959   \n",
       "12  AG News  ReplaceWithSynonymsNoise           SVM    0.5575  0.558023   \n",
       "5   AG News  ReplaceWithSynonymsNoise  TARSZeroShot    0.7425  0.713238   \n",
       "11  AG News     ShuffleSentencesNoise           SVM    0.5575  0.556953   \n",
       "4   AG News     ShuffleSentencesNoise  TARSZeroShot    0.7500  0.721180   \n",
       "\n",
       "    Precision  Recall  \n",
       "8    0.552725  0.5500  \n",
       "1    0.685400  0.7350  \n",
       "9    0.568250  0.5625  \n",
       "2    0.701125  0.7325  \n",
       "10   0.547450  0.5425  \n",
       "3    0.704000  0.7375  \n",
       "7    0.561400  0.5575  \n",
       "0    0.722200  0.7525  \n",
       "13   0.557275  0.5525  \n",
       "6    0.708975  0.7450  \n",
       "12   0.562525  0.5575  \n",
       "5    0.712600  0.7425  \n",
       "11   0.561400  0.5575  \n",
       "4    0.720750  0.7500  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_orchestrator.evaluate_with_training(ag_news_train, ag_news_sample, noise_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926c0f6d63da4b4fbc6693865080633d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7c9d1a33634b1abea7ad059b34329f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bafe62949d48db9163db9b5a2bf347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Noise Strategies:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_orchestrator\u001b[38;5;241m.\u001b[39mevaluate_with_training(ag_news_train, ag_news_sample, noise_strategies)\n",
      "File \u001b[0;32m~/githubRepos/text_clustering/src/classification_pipeline.py:120\u001b[0m, in \u001b[0;36mClassificationPipelineOrchestrator.evaluate_with_training\u001b[0;34m(self, training_data, documents_dict, noise_strategies)\u001b[0m\n\u001b[1;32m    117\u001b[0m train_x, train_y \u001b[38;5;241m=\u001b[39m training_data[dataset_name]\n\u001b[1;32m    118\u001b[0m model\u001b[38;5;241m.\u001b[39mfit_model(train_x, train_y)\n\u001b[0;32m--> 120\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(noisy_documents, true_labels)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Add metadata\u001b[39;00m\n\u001b[1;32m    123\u001b[0m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/githubRepos/text_clustering/src/classification_models.py:38\u001b[0m, in \u001b[0;36mIClasificationModel.evaluate\u001b[0;34m(self, documents, true_labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents, true_labels: pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_classes(documents)\n\u001b[1;32m     39\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(predictions, true_labels)\n\u001b[1;32m     40\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(predictions, true_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/githubRepos/text_clustering/src/classification_models.py:470\u001b[0m, in \u001b[0;36mTARSFewShotModel.predict_classes\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    468\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m sentence \u001b[38;5;241m=\u001b[39m Sentence(prompt)\n\u001b[0;32m--> 470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(sentence)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mlabels:\n\u001b[1;32m    472\u001b[0m     lbl_str \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/models/tars_model.py:924\u001b[0m, in \u001b[0;36mTARSClassifier.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, label_threshold, multi_label, force_label)\u001b[0m\n\u001b[1;32m    921\u001b[0m         labels_to_sentence[label] \u001b[38;5;241m=\u001b[39m tars_sentence\n\u001b[1;32m    922\u001b[0m     all_labels_to_sentence\u001b[38;5;241m.\u001b[39mappend(labels_to_sentence)\n\u001b[0;32m--> 924\u001b[0m loss_and_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtars_model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    925\u001b[0m     tars_sentences,\n\u001b[1;32m    926\u001b[0m     label_name\u001b[38;5;241m=\u001b[39mlabel_name,\n\u001b[1;32m    927\u001b[0m     mini_batch_size\u001b[38;5;241m=\u001b[39mmini_batch_size,\n\u001b[1;32m    928\u001b[0m     return_loss\u001b[38;5;241m=\u001b[39mreturn_loss,\n\u001b[1;32m    929\u001b[0m )\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_loss:\n\u001b[1;32m    932\u001b[0m     overall_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_and_count[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/nn/model.py:880\u001b[0m, in \u001b[0;36mDefaultClassifier.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# pass data points through network and decode\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m data_point_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_data_points(batch, data_points)\n\u001b[1;32m    881\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(data_point_tensor)\n\u001b[1;32m    882\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_scores(scores, data_points)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/nn/model.py:745\u001b[0m, in \u001b[0;36mDefaultClassifier._encode_data_points\u001b[0;34m(self, sentences, data_points)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_data_points\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: \u001b[38;5;28mlist\u001b[39m[DT], data_points: \u001b[38;5;28mlist\u001b[39m[DT2]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# embed sentences\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_embed_sentence:\n\u001b[0;32m--> 745\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39membed(sentences)\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# get a tensor of data points\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     data_point_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embedding_for_data_point(data_point) \u001b[38;5;28;01mfor\u001b[39;00m data_point \u001b[38;5;129;01min\u001b[39;00m data_points])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/embeddings/base.py:51\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     48\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_embeddings_internal(data_points)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/embeddings/transformer.py:730\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_embeddings_internal\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: \u001b[38;5;28mlist\u001b[39m[Sentence]):\n\u001b[0;32m--> 730\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_tensors(sentences, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_device)\n\u001b[1;32m    731\u001b[0m     gradient_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39menable_grad() \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gradient_context:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/embeddings/transformer.py:538\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings.prepare_tensors\u001b[0;34m(self, sentences, device)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    531\u001b[0m     [\n\u001b[1;32m    532\u001b[0m         sentences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhas_metadata(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    533\u001b[0m         sentences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhas_metadata(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    534\u001b[0m     ]\n\u001b[1;32m    535\u001b[0m ):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe embedding \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m set as metadata for all sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_transformer_model_inputs(sentences, offsets, lengths, flair_tokens, device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/flair/embeddings/transformer.py:554\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings.__build_transformer_model_inputs\u001b[0;34m(self, sentences, offsets, sentence_lengths, flair_tokens, device)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     tokenizer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m batch_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    555\u001b[0m     [[t\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m flair_tokens],\n\u001b[1;32m    556\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    557\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_long_sentences,\n\u001b[1;32m    558\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[1;32m    559\u001b[0m     padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mMAX_LENGTH \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_max_length \u001b[38;5;28;01melse\u001b[39;00m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST,\n\u001b[1;32m    560\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs,\n\u001b[1;32m    562\u001b[0m )\n\u001b[1;32m    564\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    565\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2887\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2887\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2975\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2972\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2973\u001b[0m         )\n\u001b[1;32m   2974\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2976\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2977\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2978\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2979\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2980\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2981\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2982\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2983\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2984\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   2985\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2986\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2987\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2988\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2989\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2990\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2991\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2992\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2993\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[1;32m   2994\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2995\u001b[0m     )\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2998\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2999\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3018\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3177\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3167\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3168\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3169\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3170\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3175\u001b[0m )\n\u001b[0;32m-> 3177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3178\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3179\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   3180\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   3181\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   3182\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3183\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   3184\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   3185\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3186\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   3187\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3188\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3189\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3190\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3191\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3192\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3193\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3194\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3195\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[1;32m   3196\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3197\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 539\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    540\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    541\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    542\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    543\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    553\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    563\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_orchestrator.evaluate_with_training(ag_news_train, ag_news_sample, noise_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_modelling_orchestrator.evaluate(ag_news, noise_strategies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
