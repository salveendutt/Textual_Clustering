{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from topic_modeling import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "random.seed(42)\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_random_word(text, probability=0.1):\n",
    "#     \"\"\"Add random words from WordNet to the text with a certain probability.\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return text\n",
    "    \n",
    "#     # Preload synsets to avoid repeated expensive calls\n",
    "#     synsets = list(wordnet.all_synsets())\n",
    "\n",
    "#     def get_random_wordnet_word():\n",
    "#         \"\"\"Get a random word from WordNet, ensuring it's a single-word noun or verb.\"\"\"\n",
    "#         while True:\n",
    "#             random_synset = random.choice(synsets)  # Avoid regenerating the list\n",
    "#             word = random_synset.lemmas()[0].name()  # Get first lemma of the synset\n",
    "#             if '_' not in word:  # Ensure single-word output\n",
    "#                 return word\n",
    "            \n",
    "#     words = text.split()\n",
    "#     modified_words = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         modified_words.append(word)\n",
    "#         if random.random() < probability:\n",
    "#             random_word = get_random_wordnet_word()\n",
    "#             modified_words.append(random_word)\n",
    "    \n",
    "#     return ' '.join(modified_words)\n",
    "\n",
    "\n",
    "# def add_random_character(text, probability=0.1):\n",
    "#     \"\"\"Add random characters to words in text with a certain probability.\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return text\n",
    "    \n",
    "#     words = text.split()\n",
    "#     modified_words = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         if len(word) > 2 and random.random() < probability:\n",
    "#             # Choose a random position to insert the character\n",
    "#             position = random.randint(1, len(word) - 1)\n",
    "#             # Choose a random lowercase letter\n",
    "#             random_char = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "#             word = word[:position] + random_char + word[position:]\n",
    "#         modified_words.append(word)\n",
    "    \n",
    "#     return ' '.join(modified_words)\n",
    "\n",
    "# def random_word_deletion(text, probability=0.1):\n",
    "#     \"\"\"Randomly delete words from text with a certain probability.\"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return text\n",
    "    \n",
    "#     words = text.split()\n",
    "#     if len(words) <= 3:  # Don't delete if text is too short\n",
    "#         return text\n",
    "    \n",
    "#     modified_words = []\n",
    "    \n",
    "#     for word in words:\n",
    "#         if random.random() >= probability:\n",
    "#             modified_words.append(word)\n",
    "    \n",
    "#     # Ensure we don't delete all words\n",
    "#     if not modified_words:\n",
    "#         modified_words = [random.choice(words)]\n",
    "    \n",
    "#     return ' '.join(modified_words)\n",
    "\n",
    "def shuffle_sentences(text, probability=0.5):\n",
    "    \"\"\"Reorder sentences within a document with a certain probability.\"\"\"\n",
    "    if not isinstance(text, str) or len(text) < 10:\n",
    "        return text\n",
    "    \n",
    "    if random.random() < probability:\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) > 1:\n",
    "            random.shuffle(sentences)\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def replace_with_synonym(text, probability=0.1):\n",
    "    \"\"\"Replace words with their synonyms with a certain probability.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    modified_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isalpha() and len(word) > 3 and random.random() < probability:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                modified_words.append(random.choice(synonyms))\n",
    "            else:\n",
    "                modified_words.append(word)\n",
    "        else:\n",
    "            modified_words.append(word)\n",
    "    \n",
    "    return ' '.join(modified_words)\n",
    "\n",
    "def create_adversarial_examples(text, probability=0.1, fallback_to_predefined=True):\n",
    "    \"\"\"\n",
    "    Create adversarial examples by replacing words with their antonyms from WordNet.\n",
    "    If WordNet doesn't have antonyms for a word and fallback_to_predefined is True,\n",
    "    uses a predefined dictionary of common antonyms.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Common antonym pairs as fallback\n",
    "    predefined_antonyms = {\n",
    "        'good': 'bad', 'bad': 'good', 'high': 'low', 'low': 'high',\n",
    "        'increase': 'decrease', 'decrease': 'increase', 'positive': 'negative',\n",
    "        'negative': 'positive', 'success': 'failure', 'failure': 'success',\n",
    "        'true': 'false', 'false': 'true', 'right': 'wrong', 'wrong': 'right',\n",
    "        'happy': 'sad', 'sad': 'happy', 'up': 'down', 'down': 'up',\n",
    "        'big': 'small', 'small': 'big', 'fast': 'slow', 'slow': 'fast',\n",
    "        'strong': 'weak', 'weak': 'strong', 'rich': 'poor', 'poor': 'rich',\n",
    "        'win': 'lose', 'lose': 'win', 'best': 'worst', 'worst': 'best',\n",
    "        'approve': 'reject', 'reject': 'approve', 'agree': 'disagree', 'disagree': 'agree',\n",
    "        'accept': 'deny', 'deny': 'accept', 'buy': 'sell', 'sell': 'buy'\n",
    "    }\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    modified_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isalpha() and len(word) > 2 and random.random() < probability:\n",
    "            word_lower = word.lower()\n",
    "            antonyms = get_antonyms(word_lower)\n",
    "            \n",
    "            if antonyms:\n",
    "                # Use WordNet antonym\n",
    "                replacement = random.choice(antonyms)\n",
    "                \n",
    "                # Keep the original capitalization pattern\n",
    "                if word[0].isupper():\n",
    "                    replacement = replacement.capitalize()\n",
    "                \n",
    "                modified_words.append(replacement)\n",
    "            elif fallback_to_predefined and word_lower in predefined_antonyms:\n",
    "                # Fallback to predefined antonym if no WordNet antonyms exist\n",
    "                replacement = predefined_antonyms[word_lower]\n",
    "                \n",
    "                # Keep the original capitalization pattern\n",
    "                if word[0].isupper():\n",
    "                    replacement = replacement.capitalize()\n",
    "                \n",
    "                modified_words.append(replacement)\n",
    "            else:\n",
    "                modified_words.append(word)\n",
    "        else:\n",
    "            modified_words.append(word)\n",
    "    \n",
    "    return ' '.join(modified_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache a filtered list of simple words\n",
    "_CACHED_WORDS = None\n",
    "\n",
    "def _build_word_cache():\n",
    "    words = []\n",
    "    # Use noun synsets only for faster loading\n",
    "    for synset in list(wordnet.all_synsets()):\n",
    "        for lemma in synset.lemmas():\n",
    "            word = lemma.name()\n",
    "            if '_' not in word:  # only simple words\n",
    "                words.append(word)\n",
    "    return list(set(words))  # unique words\n",
    "\n",
    "def add_random_word(text, probability=0.1):\n",
    "    \"\"\"Add random words from WordNet to the text with a certain probability.\"\"\"\n",
    "    global _CACHED_WORDS\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Build cache on first use\n",
    "    if _CACHED_WORDS is None:\n",
    "        _CACHED_WORDS = _build_word_cache()\n",
    "\n",
    "    words = text.split()\n",
    "    modified_words = []\n",
    "\n",
    "    for word in words:\n",
    "        modified_words.append(word)\n",
    "        if random.random() < probability:\n",
    "            random_word = random.choice(_CACHED_WORDS)\n",
    "            modified_words.append(random_word)\n",
    "\n",
    "    return ' '.join(modified_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_test = pd.read_csv('../data/AG News/test.csv')\n",
    "ag_train = pd.read_csv('../data/AG News/train.csv')\n",
    "\n",
    "# Define the mapping\n",
    "class_mapping = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Science\"}\n",
    "\n",
    "# Apply the mapping to the class column\n",
    "ag_test['Class'] = ag_test['Class Index'].replace(class_mapping)\n",
    "ag_train['Class'] = ag_train['Class Index'].replace(class_mapping)\n",
    "\n",
    "ag_news_baseline = ag_test['Description']\n",
    "# ag_news_char_insertion = ag_test['Description'].apply(add_random_character)\n",
    "# ag_news_word_deletion = ag_test['Description'].apply(random_word_deletion)\n",
    "# ag_news_shuffle_sent = ag_test['Description'].apply(shuffle_sentences)\n",
    "# ag_news_adversarial = ag_test['Description'].apply(create_adversarial_examples)\n",
    "# ag_news_synonym = ag_test['Description'].apply(replace_with_synonym)\n",
    "# ag_news_word_insertion = ag_test['Description'].apply(add_random_word)\n",
    "# ag_news_combined = ag_test['Description'].apply(replace_with_synonym) \\\n",
    "#     .apply(create_adversarial_examples) \\\n",
    "#     .apply(add_random_word) \\\n",
    "#     .apply(shuffle_sentences) \\\n",
    "#     .apply(random_word_deletion) \\\n",
    "#     .apply(add_random_character)\n",
    "\n",
    "ag_news_true_labels = ag_test['Class Index']\n",
    "\n",
    "ag_news = {\n",
    "    \"AG News\":(ag_news_baseline, ag_news_true_labels),\n",
    "    # \"AG News Added Random Chars\":(ag_news_char_insertion, ag_news_true_labels),\n",
    "    # \"AG News Random Word Deletion\":(ag_news_word_deletion, ag_news_true_labels),\n",
    "    # \"AG News Shuffled Sentances\":(ag_news_shuffle_sent, ag_news_true_labels),\n",
    "    # \"AG News Adversarial\":(ag_news_adversarial, ag_news_true_labels),\n",
    "    # \"AG News Synonym\":(ag_news_synonym, ag_news_true_labels),\n",
    "    # \"AG News Added Random Word\":(ag_news_word_insertion, ag_news_true_labels),\n",
    "    # \"AG News Noisy\":(ag_news_combined, ag_news_true_labels),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LDA_4', 'LSI_4', 'NMF_4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator = TopicModelingPipelineOrchestrator()\n",
    "\n",
    "orchestrator.add_models_grid(\n",
    "    model_types=['LDA', 'LSI', 'NMF'],\n",
    "    param_grid={'n_topics': [4]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models on dataset: AG News\n"
     ]
    }
   ],
   "source": [
    "results = orchestrator.evaluate(ag_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ARI Score</th>\n",
       "      <th>Topic Coherence</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Reconstruction Error</th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA_4</td>\n",
       "      <td>0.154719</td>\n",
       "      <td>0.533687</td>\n",
       "      <td>0.389324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LDA_4</td>\n",
       "      <td>AG News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSI_4</td>\n",
       "      <td>0.111348</td>\n",
       "      <td>0.631264</td>\n",
       "      <td>0.564310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LSI_4</td>\n",
       "      <td>AG News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NMF_4</td>\n",
       "      <td>0.405050</td>\n",
       "      <td>0.722596</td>\n",
       "      <td>0.473902</td>\n",
       "      <td>86.28456</td>\n",
       "      <td>NMF_4</td>\n",
       "      <td>AG News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ARI Score  Topic Coherence  Cosine Similarity  Reconstruction Error  \\\n",
       "0  LDA_4   0.154719         0.533687           0.389324                   NaN   \n",
       "1  LSI_4   0.111348         0.631264           0.564310                   NaN   \n",
       "2  NMF_4   0.405050         0.722596           0.473902              86.28456   \n",
       "\n",
       "   Model  Dataset  \n",
       "0  LDA_4  AG News  \n",
       "1  LSI_4  AG News  \n",
       "2  NMF_4  AG News  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
