{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN WHEN RUNNING FOR THE FIRST TIME\n",
    "# !ollama pull gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from langchain_ollama import OllamaLLM\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_ollama import OllamaLLM\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(df, predicted_col='Predicted Topic Index', true_col='Class Index'):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics for news topic prediction.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the predictions and true labels\n",
    "        predicted_col: Column name for predicted labels (text format)\n",
    "        true_col: Column name for true labels (numeric format)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of calculated metrics\n",
    "    \"\"\"\n",
    "    true_labels = df[true_col].tolist()\n",
    "    predicted_labels = df[predicted_col].tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "    # Print the metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topics(input_df, topics, mapping, llm=None, output_path=None):\n",
    "    if llm is None:\n",
    "        raise ValueError(\"LLM instance must be provided\")\n",
    "    if output_path is None:\n",
    "        raise ValueError(\"Output path must be provided\")\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # Initialize Topic column with 'Unknown'\n",
    "    df['Predicted Topic'] = 'Unknown'\n",
    "    \n",
    "    # Process each news item individually with progress bar\n",
    "    for idx in tqdm(range(len(df)), desc=\"Assigning topics\"):\n",
    "        news_item = df.iloc[idx]['Description']\n",
    "        \n",
    "        # Generate prompt for current item\n",
    "        prompt_assigning_prompt = f'''You are provided with news and helping to classify them based on the topics.\n",
    "Please assign the news to the topics provided. Return only the name of the topic for the respective news.\n",
    "News can be found in tripletick block: ```{news_item}```\n",
    "Topics to choose from: {topics}\n",
    "Please return ONLY the topic name. DO NOT OUTPUT any additional text, quotes, or formatting. Only 1 topic  should be returned.'''\n",
    "        \n",
    "        try:\n",
    "            result = llm.invoke(prompt_assigning_prompt)\n",
    "            # Clean the result\n",
    "            result = result.strip()\n",
    "            if result.startswith('```') and result.endswith('```'):\n",
    "                result = result[3:-3].strip()\n",
    "            \n",
    "            # Update the DataFrame with the assigned topic\n",
    "            df.iloc[idx, df.columns.get_loc('Predicted Topic')] = result\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {idx}: {str(e)}\")\n",
    "            print(f\"Result received: {result}\")\n",
    "            continue\n",
    "    df['Predicted Topic Index'] = df['Predicted Topic'].map(mapping)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nFinal results saved: {output_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_per_class = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_train = pd.read_csv('../data/AG News/train.csv')\n",
    "\n",
    "class_to_text_mapping = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Science\"}\n",
    "text_to_class_mapping = {'World': 1, 'Sports': 2, 'Business': 3, 'Science': 4}\n",
    "ag_train['Class'] = ag_train['Class Index'].map(class_to_text_mapping)\n",
    "\n",
    "# CREATING A SAMPLE TRAIN SET\n",
    "ag_train_world = ag_train[ag_train['Class Index'] == 1].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_sports = ag_train[ag_train['Class Index'] == 2].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_business = ag_train[ag_train['Class Index'] == 3].sample(sample_size_per_class, random_state=42)\n",
    "ag_train_science = ag_train[ag_train['Class Index'] == 4].sample(sample_size_per_class, random_state=42)\n",
    "\n",
    "# Combine the four dataframes of different categories\n",
    "ag_train = pd.concat([ag_train_world, \n",
    "                            ag_train_sports, \n",
    "                            ag_train_business, \n",
    "                            ag_train_science], \n",
    "                           ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "ag_train = ag_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Reset the index\n",
    "ag_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "topics_ag_news = ag_train['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_output = assign_topics(\n",
    "    ag_train,\n",
    "    topics=str(topics_ag_news),\n",
    "    mapping=text_to_class_mapping,\n",
    "    llm=llm,\n",
    "    output_path='../data/AG News/train_from_llm.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_output = pd.read_csv('../data/AG News/train_from_llm.csv')\n",
    "ag_metrics = calculate_metrics(ag_news_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_full = pd.read_csv('../data/BBC News/BBC News Train.csv')\n",
    "bbc_full = bbc_full.rename(columns={'Category': 'Class', 'Text': 'Description'})\n",
    "\n",
    "bbc_class_to_index = {\n",
    "    'business': 1,\n",
    "    'tech': 2,\n",
    "    'entertainment': 3,\n",
    "    'politics': 4,\n",
    "    'sport': 5\n",
    "}\n",
    "\n",
    "bbc_index_to_class = {\n",
    "    1: 'business',\n",
    "    2: 'tech',\n",
    "    3: 'entertainment',\n",
    "    4: 'politics',\n",
    "    5: 'sport'\n",
    "}\n",
    "\n",
    "bbc_full['Class Index'] = bbc_full['Class'].map(bbc_class_to_index)\n",
    "\n",
    "\n",
    "bbc_train_business = bbc_full[bbc_full['Class Index'] == 1].sample(sample_size_per_class, random_state=42)\n",
    "bbc_train_tech = bbc_full[bbc_full['Class Index'] == 2].sample(sample_size_per_class, random_state=42)\n",
    "bbc_train_entertainment = bbc_full[bbc_full['Class Index'] == 3].sample(sample_size_per_class, random_state=42)\n",
    "bbc_train_politics = bbc_full[bbc_full['Class Index'] == 4].sample(sample_size_per_class, random_state=42)\n",
    "bbc_train_sport = bbc_full[bbc_full['Class Index'] == 5].sample(sample_size_per_class, random_state=42)\n",
    "\n",
    "# Create a test set by excluding the training samples\n",
    "bbc_train_indices = pd.concat([\n",
    "    bbc_train_business,\n",
    "    bbc_train_tech, \n",
    "    bbc_train_entertainment,\n",
    "    bbc_train_politics,\n",
    "    bbc_train_sport\n",
    "]).index\n",
    "# Create test set by excluding training indices\n",
    "bbc_test = bbc_full.drop(bbc_train_indices).reset_index(drop=True)\n",
    "bbc_test.to_csv('../data/BBC News/test.csv', index=False)\n",
    "\n",
    "# Combine the four dataframes of different categories\n",
    "bbc_train = pd.concat([\n",
    "    bbc_train_business, \n",
    "    bbc_train_tech, \n",
    "    bbc_train_entertainment, \n",
    "    bbc_train_politics, \n",
    "    bbc_train_sport\n",
    "], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "bbc_train = bbc_train.sample(frac=1, random_state=42)\n",
    "bbc_train = bbc_train.reset_index(drop=True)\n",
    "\n",
    "topics_bbc_news = bbc_train['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news_output = assign_topics(\n",
    "    bbc_train,\n",
    "    topics=str(topics_bbc_news),\n",
    "    llm=llm,\n",
    "    mapping=bbc_class_to_index,\n",
    "    output_path='../data/BBC News/train_from_llm_2.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news_output = pd.read_csv('../data/BBC News/train_from_llm.csv')\n",
    "bbc_metrics = calculate_metrics(bbc_news_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 News Group Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 18846\n",
      "Original dataset size without duplicates: 18371\n",
      "Trimmed dataset size (The one we will use): 8000\n",
      "Training set size: 400\n",
      "Test set size: 7600\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Get category names and create mappings\n",
    "categories = newsgroups.target_names\n",
    "newsgroup20_class_to_index = {cat: i+1 for i, cat in enumerate(categories)}\n",
    "newsgroup20_index_to_class = {i+1: cat for i, cat in enumerate(categories)}\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "newsgroup_data = pd.DataFrame({\n",
    "    'Description': newsgroups.data,\n",
    "    'Class Index': newsgroups.target + 1  # Adding 1 to match our 1-indexed scheme\n",
    "})\n",
    "# Map class indices to text labels\n",
    "newsgroup_data['Class'] = newsgroup_data['Class Index'].map(newsgroup20_index_to_class)\n",
    "newsgroup_data_no_duplicates = newsgroup_data.drop_duplicates()\n",
    "\n",
    "# To reduce the dataset size, we will sample a fixed number of samples per class\n",
    "max_samples_per_class_dataset_trim = 400 # 400 * 20 = 8000 total samples\n",
    "newsgroup_samples = []\n",
    "for class_idx in range(1, len(categories) + 1):\n",
    "    class_data = newsgroup_data_no_duplicates[newsgroup_data_no_duplicates['Class Index'] == class_idx]\n",
    "    if len(class_data) >= max_samples_per_class_dataset_trim:\n",
    "        sampled = class_data.sample(max_samples_per_class_dataset_trim, random_state=42)\n",
    "        newsgroup_samples.append(sampled)\n",
    "    else:\n",
    "        print(f\"Warning: Class {class_idx} has only {len(class_data)} samples\")\n",
    "        newsgroup_samples.append(class_data)\n",
    "# Combine all samples\n",
    "newsgroup = pd.concat(newsgroup_samples, ignore_index=True)\n",
    "# Shuffle the data\n",
    "newsgroup = newsgroup.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "newsgroup_samples = []\n",
    "for class_idx in range(1, len(categories) + 1):\n",
    "    class_data = newsgroup[newsgroup['Class Index'] == class_idx]\n",
    "    if len(class_data) >= sample_size_per_class:\n",
    "        sampled = class_data.sample(sample_size_per_class, random_state=42)\n",
    "        newsgroup_samples.append(sampled)\n",
    "    else:\n",
    "        print(f\"Warning: Class {class_idx} has only {len(class_data)} samples\")\n",
    "        newsgroup_samples.append(class_data)\n",
    "# Combine all samples\n",
    "newsgroup_train = pd.concat(newsgroup_samples, ignore_index=True)\n",
    "# Shuffle the data\n",
    "newsgroup_train = newsgroup_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Get unique topics\n",
    "topics_newsgroup20 = newsgroup_train['Class'].unique()\n",
    "\n",
    "newsgroup_test = newsgroup.merge(\n",
    "    newsgroup_train, \n",
    "    on=list(newsgroup.columns), \n",
    "    how='left', \n",
    "    indicator=True\n",
    ")\n",
    "newsgroup_test = newsgroup_test[newsgroup_test['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "print(f\"Original dataset size: {len(newsgroup_data)}\")\n",
    "print(f\"Original dataset size without duplicates: {len(newsgroup_data_no_duplicates)}\")\n",
    "print(f\"Trimmed dataset size (The one we will use): {len(newsgroup)}\")\n",
    "print(f\"Training set size: {len(newsgroup_train)}\")\n",
    "print(f\"Test set size: {len(newsgroup_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../data/20 Newsgroups', exist_ok=True)\n",
    "newsgroup_test.to_csv('../data/20 Newsgroups/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599642762a824e6b8240cb08e592db06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning topics:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results saved: ../data/20 Newsgroups/train_from_llm.csv\n"
     ]
    }
   ],
   "source": [
    "# Assign topics using the LLM\n",
    "newsgroup_output = assign_topics(\n",
    "    newsgroup_train,\n",
    "    topics=str(topics_newsgroup20),\n",
    "    mapping=newsgroup20_class_to_index,\n",
    "    llm=llm,\n",
    "    output_path='../data/20 Newsgroups/train_from_llm.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5850\n",
      "F1 Score: 0.5896\n",
      "Precision: 0.6867\n",
      "Recall: 0.5850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "newsgroup_output = pd.read_csv('../data/20 Newsgroups/train_from_llm.csv')\n",
    "newsgroup_output['Predicted Topic Index'] = newsgroup_output['Predicted Topic Index'].fillna(-1)\n",
    "newsgroup_output.to_csv('../data/20 Newsgroups/train_from_llm.csv', index=False)\n",
    "# Fill NA values in Predicted Topic Index with -1\n",
    "newsgroup_metrics = calculate_metrics(newsgroup_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
